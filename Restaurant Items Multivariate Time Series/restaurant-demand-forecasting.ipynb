{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Multivariate Time Series Restaurant Demand forecasting"},{"metadata":{},"cell_type":"markdown","source":" ## **Business Problem and Limitations -**\n\n Our dataset is for restaurant sales for Tuesday and Wednesday, both lunch and dinner time.\n \nThere are few instances of 'To-Go' orders like Uber Eats in this dataset.\n \nTypical lunch hour is 11:30 AM-2:00 PM, and dinner hour is 6:30 PM-10:00 PM\n \nThe data set is just for Tuesday and Wednesday.\nWe needs to expand and randomize the data for min. of 6 months (Jan. 2019 to June 2019) for all days of the week.\n\nA typical restaurant has high covers (number of customers) on Wednesday, Weekend Dinner, followed by Weekend Lunch, and then relatively low covers for Monday to Friday Lunch.\n \nThe data expansion/randomization should follow the above pattern for the number of customers.\n\n### **Our Gole -**\nPredict the top 'Menu Item' and 'Item Qty' for Lunch and Dinner. \nThese predictions need to be for future dates (Monday to Sunday, July 1st to July 7th)"},{"metadata":{},"cell_type":"markdown","source":"## **Implementation -**"},{"metadata":{},"cell_type":"markdown","source":"## **1) Data Preparation and Preprocessing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_row',None)\nsns.set_style('darkgrid') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_excel('../input/restaurant-dataxlsx/Data.xlsx')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping unnecessary columns\ndf = df.drop(['StoreCode','DTS','Month','Date','Year','TicketCode'],axis = 1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cheking data type of all columns\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing null by '0' as there is no peoples when food orderd online.\ndf['PartySize'] = df['PartySize'].replace(['na'],0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalizing text \ndf['MenuCateogry'] = df['MenuCateogry'].str.capitalize()\ndf['MenuItem'] = df['MenuItem'].str.capitalize()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After removing unnecessary columns and imputing null values of the dataset, as shown above, We are ready for data preparation as per the pattern provided above."},{"metadata":{},"cell_type":"markdown","source":"As in our dataset, Wednesday is given as weekend and it's only available weekend data, So we are using it as it is for further data preparation.\nNow we are dividing data into two groups first one is a weekday and another one is the weekend."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dividing data based on Weekday\nweekday = df[df['Day Type']=='Weekday']\nprint(weekday.shape)\nweekday.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dividing data based on Weekend\nweekend = df[df['Day Type']=='Weekend']\nprint(weekend.shape)\nweekend.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating data for each day in a week as per the limitations and information we have."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Monday - WeekDay\nmonday = weekday.copy()\nmonday['Day'] = weekday['Day'].str.replace('Tuesday','Monday') \n\n# Tuseday - WeekDay\ntuesday = weekday.copy() \n\n# Wednesday - WeekEnd\nwednesday = weekend.copy()\n\n# Thusday - WeekDay\nthursday = weekday.copy()                                         \nthursday['Day'] = thursday['Day'].str.replace('Tuesday','Thursday')\n\n# Friday - WeekDay\nfriday = weekday.copy()                                          \nfriday['Day'] = friday['Day'].str.replace('Tuesday','Friday')\n\n# Saturday - WeekEnd\nsaturday = weekend.copy()                                        \nsaturday['Day'] = saturday['Day'].str.replace('Wednesday','Saturday')\n\n# Sunday - WeekEnd\nsunday = weekend.copy()                                          \nsunday['Day'] = sunday['Day'].str.replace('Wednesday','Sunday')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating data for one week \nweek = []\nweek = pd.concat([tuesday,wednesday,thursday,friday,saturday,sunday,monday,],axis = 0)\nprint(week.shape)\nweek.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After creating data for one week now we have to expanding it for 6 months with the same pattern. So we are writing a function for it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating data for 6 Months with help of above data\nmonths = week.copy()\nx = 0\nwhile x < 25:\n    months = pd.concat([months,week],axis = 0)\n    x = x+1\nmonths.reset_index(drop=True, inplace=True)\nprint(months.shape)\nmonths.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"months.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating dates for dataframe.\no = pd.date_range(start='1/1/2019', periods=(len(months)/100), freq='D')\ndate = []\nfor i in o:\n    for j in range(100):\n        date.append(i)\ndate = pd.DataFrame(date,columns = ['Date'])\ndate.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concating Dates and Months Dataframe\nfinal = pd.concat([date,months],axis = 1)\nfinal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing Columns Postions for better understanding\nfinal = final[['Date', 'Shift', 'Day Type', 'Day', 'PartySize', 'MenuCateogry','MenuItem', 'ItemPrice', 'ItemQty']]\nfinal = final.iloc[:18100,:]\nfinal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Here our Data Preparation is finally completed, we maintain all patterns and consider other constraints so that data matches real-world data.\n### Now we use final data for further Process."},{"metadata":{"trusted":true},"cell_type":"code","source":"df  = final\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **2) Visualization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting Two Columns for visualization purpose.\nproduct_1 = df[['MenuItem','ItemQty']]\n\n# Combining two rows 'MenuItem' and 'ItemQty' for Analysis and multiplying based on ItemQty\nproduct = product_1.loc[product_1.index.repeat(product_1.ItemQty)].reset_index(drop=True)\nproduct = product[['MenuItem']]\nproduct.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Word Cloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"#pip install WordCloud  # Kindly install WordCloud package for furthur process \n# Joinining all the reviews into single paragraph  \nspeaker_rev_string1 = \" \".join(product['MenuItem'])\nfrom wordcloud import WordCloud\nwordcloud_sp = WordCloud(width=6000,height=1800).generate(speaker_rev_string1)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.imshow(wordcloud_sp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Barplot"},{"metadata":{"trusted":true},"cell_type":"code","source":"p = pd.DataFrame(product_1.groupby(['MenuItem']).sum())\np = p.reset_index()\np.sort_values(by=['ItemQty'], inplace=True,ascending=False)\nplt.figure(figsize=(35,8))\nchart = sns.barplot(x=\"MenuItem\", y=\"ItemQty\", data=p)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3) Model Building\nIn order to build a model firstly, we are modifying some arrangements of the table to facilitate multivariate forecasting."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[['Date','Shift','MenuItem','ItemQty']]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing data to use to make Cross Table\nnew = df.loc[df.index.repeat(df.ItemQty)]\nnew = new[['Date','Shift','MenuItem']]\nnew.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shifting Table\ntable = pd.DataFrame(pd.crosstab(new.Date,[new.Shift,new.MenuItem]))\ntable.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalizing Table Names.\ntable.columns = table.columns.map('{0[1]}-{0[0]}'.format) \nprint(table.shape)\ntable.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking Behavior of above Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(5,60))\nplt.rcParams[\"figure.figsize\"] = [35, 8]\ntable.plot(legend = False)\n# We Can see that data follows seasonality","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting Data into Train and Test.**\n\nFor the time series model we can not split our data randomly we should follow seasonilty pattern of data while splitting. So we are splitting as follows."},{"metadata":{"trusted":true},"cell_type":"code","source":"Train = table[:int(0.85*(len(table)))]\nTest = table[int(0.85*(len(table))):]\nprint(Train.shape,Test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this data, we are using a Data-driven time series model such as smoothing techniques.\n\nImporting libraries necessary for model building"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.holtwinters import SimpleExpSmoothing \nfrom statsmodels.tsa.holtwinters import Holt \nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing \nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we are building different models for each item of the menu and make a data frame of respective RMSE to detect which model is best among all.**"},{"metadata":{},"cell_type":"markdown","source":"### 1st Model:-\n**Winter Exponential Smoothing with Additive Seasonality and Additive Trend Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"p = []\nfor i in table.columns:\n    hwe_model_add_add = ExponentialSmoothing(Train[i],seasonal=\"add\",trend=\"add\",seasonal_periods=7).fit()\n    pred_hwe_add_add = hwe_model_add_add.predict(start = Test.index[0],end = Test.index[-1])\n    rmse_hwe_add_add = np.sqrt(np.mean((Test[i]-pred_hwe_add_add)**2))\n    p.append(round(rmse_hwe_add_add,3))\np = pd.DataFrame(p, columns = ['Winter_Exponential_Smoothing_RMSE'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2nd Model:- \n**Holt Method Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"q = []\nfor j in table.columns:\n    hw_model = Holt(Train[j]).fit()\n    pred_hw = hw_model.predict(start = Test.index[0],end = Test.index[-1])\n    rmse_hw = np.sqrt(np.mean((Test[j]-pred_hw)**2))\n    q.append(round(rmse_hw,3)) \np['Holt method Model_RMSE']= pd.DataFrame(q, columns = ['Holt method Model_RMSE'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3rd Model:- \n**Simple Exponential Mode**"},{"metadata":{"trusted":true},"cell_type":"code","source":"r = []\nfor o in table.columns:\n    ses_model = SimpleExpSmoothing(Train[o]).fit()\n    pred_ses = ses_model.predict(start = Test.index[0],end = Test.index[-1])\n    rmse_ses = np.sqrt(np.mean((Test[o]-pred_ses)**2))\n    r.append(round(rmse_ses,3)) # 0.49\np['Simple Exponential Mode_RMSE']= pd.DataFrame(r, columns = ['Simple Exponential Mode_RMSE'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RMSE Data Frame of each model for each item."},{"metadata":{"trusted":true},"cell_type":"code","source":"p.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sum of RMSE's model wise."},{"metadata":{"trusted":true},"cell_type":"code","source":"p.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **From above we can see that Winters Exponential Smoothing with a seasonality of 7 showing less RMSE values as compare to others, So we are selecting it as the final model for our prediction.**"},{"metadata":{},"cell_type":"markdown","source":"## 4) Building Functions\nWe are building two different functions first one is to forecast the demand for all items in the menu and the second one is to forecast the demand of the top N item on the menu.\n"},{"metadata":{},"cell_type":"markdown","source":"### 1st Function:-\nForecast the demand for all items and normalize the arrangement of Dataframe in Lunch and dinner formate.\n\n**Input parameters-**\n\n***table*** - Historical Dataframe after modification.\n\n***start_date*** - First date of the period which we want to predict.\n\n**end_date** - Last date of the period which we want to predict."},{"metadata":{"trusted":true},"cell_type":"code","source":"def Daily_menu_forcasting(table,start_date,end_date):\n    da = pd.date_range(start = start_date, end = end_date , freq='D')\n    for_pred = pd.DataFrame(da,columns = ['Date'] )\n    for_pred = for_pred.set_index('Date')\n    for i in table.columns:\n        hwe_model_add_add = ExponentialSmoothing(table[i],seasonal=\"add\",trend=\"add\",seasonal_periods=7).fit()\n        pred_hwe_add_add = hwe_model_add_add.predict(start = for_pred.index[0],end = for_pred.index[-1])\n        for_pred[i]=((round(pred_hwe_add_add)).astype(int))\n    final_pred =  for_pred\n    p = pd.DataFrame(final_pred.stack())\n    p = p.reset_index()\n    p[['MenuItem','Shift']] = p.level_1.str.split(\"-\",expand=True,)\n    p = p.rename(columns={0: \"ItemQty\"})\n    p = p[['Date','Shift','MenuItem',\"ItemQty\"]]\n    p = p[p['ItemQty'] != 0]\n    # Makind Dataframe with dinner and lunch columns\n    new = p.loc[p.index.repeat(p.ItemQty)]\n    f = pd.DataFrame(pd.crosstab([new.Date,new.MenuItem],[new.Shift]))\n    f = f.reset_index()\n\n    # Shorting Data Frame on the basis top item\n    f['Total orders of Day'] = f.Dinner + f.Lunch\n    f = f.sort_values(['Date', 'Total orders of Day'], ascending=[True, False]).reset_index(drop= True)\n    f\n    Daily_req_FiNal_Ans = f.copy()\n    return Daily_req_FiNal_Ans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2nd Function:-\nForecast the demand for top N items and normalize the arrangement of Dataframe in Lunch and dinner formate.\n\n**Input parameters-**\n\n***table*** - Historical Dataframe after modification.\n\n***start_date*** - First date of the period which we want to predict.\n\n**end_date** - Last date of the period which we want to predict.\n\n**N** - Number of the top items we want. (default n=5)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Daily_top_menu_forcasting(table,start_date,end_date,N=5):\n    da = pd.date_range(start = start_date, end = end_date , freq='D')\n    for_pred = pd.DataFrame(da,columns = ['Date'] )\n    for_pred = for_pred.set_index('Date')\n    for i in table.columns:\n        hwe_model_add_add = ExponentialSmoothing(table[i],seasonal=\"add\",trend=\"add\",seasonal_periods=7).fit()\n        pred_hwe_add_add = hwe_model_add_add.predict(start = for_pred.index[0],end = for_pred.index[-1])\n        for_pred[i]=((round(pred_hwe_add_add)).astype(int))\n    final_pred =  for_pred\n    p = pd.DataFrame(final_pred.stack())\n    p = p.reset_index()\n    p[['MenuItem','Shift']] = p.level_1.str.split(\"-\",expand=True,)\n    p = p.rename(columns={0: \"ItemQty\"})\n    p = p[['Date','Shift','MenuItem',\"ItemQty\"]]\n    p = p[p['ItemQty'] != 0]\n    # Makind Dataframe with dinner and lunch columns\n    new = p.loc[p.index.repeat(p.ItemQty)]\n    f = pd.DataFrame(pd.crosstab([new.Date,new.MenuItem],[new.Shift]))\n    f = f.reset_index()\n\n    # Shorting Data Frame on the basis top item\n    f['Total orders of Day'] = f.Dinner + f.Lunch\n    f = f.sort_values(['Date', 'Total orders of Day'], ascending=[True, False]).reset_index(drop= True)\n    f\n    # Finding Topr product for days.\n    name =((f['Date'].astype(str)).unique()).tolist()\n    t = pd.DataFrame(columns = f.columns)\n    for i in name:\n        v = pd.DataFrame((f[f['Date']==i]).head(N))\n        t = pd.concat([t,v],axis = 0)\n    Daily_top_FiNal_Ans = t.reset_index(drop = True)\n    return(Daily_top_FiNal_Ans)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now final Demand forecasting of items on the menu for future dates (Monday to Sunday, July 1st to July 7th) is."},{"metadata":{},"cell_type":"markdown","source":"### For total items."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_menu = Daily_menu_forcasting(table,'7/1/2019','7/7/2019')\nall_menu.head(10) # top manu day wise","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For top N items."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here N = 8\ntop_8_menu = Daily_top_menu_forcasting(table,'7/1/2019','7/7/2019',8)\ntop_8_menu.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion -\nWe successfully forecast the demand for restaurant food items on the menu for Lunch and dinner for a given time period and also identify top items on the menu. This article shows one of many ways of demand forecasting implementation, according to me it is one of the best ways of handling multivariate forecasting business problems."},{"metadata":{},"cell_type":"markdown","source":"### ***Please upvote if you find this Notbook is helpful and thank you so much for giving your valuable time.***\nfor more information about multivariate time series  and data science please visit\nhttps://medium.com/@nilaydeshmukh\n\nMy Linkedin - https://www.linkedin.com/in/nilaydeshmukh/"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}